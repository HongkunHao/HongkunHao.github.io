---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
# About me
I am a second-year master student (2022 - present) at [Department of Computer Science and Engineering](https://www.cs.sjtu.edu.cn/) at [Shanghai Jiao Tong University (SJTU)](https://en.sjtu.edu.cn/). I am fortunate to be advised by [Prof. Rui Wang](https://wangruinlp.github.io/). Before that, I received the bachelor degree in Computer Science and Engineering from Shanghai Jiao Tong University (SJTU) in 2022.



# üî¨ Research

**Large Language Models**

**Neural Machine Translation**




# üî• News
- *2024.03*: üéâüéâ One paper about evaluating large language models is accepted by [NAACL 2024](https://2024.naacl.org/).



# üìù Publications 
\* denotes co-first authors
- [Rethinking Translation Memory Augmented Neural Machine Translation](https://aclanthology.org/2023.findings-acl.162/)

  **Hongkun Hao**, Guoping Huang, Lemao Liu, Zhirui Zhang, Shuming Shi, Rui Wang

  ``ACL 2023 Findings`` \| <a href='https://github.com/hongkunhao/translation_memory_augmented_NMT'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="hao-etal-2023-rethinking">BIB</button>


- [Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation](https://aclanthology.org/2023.emnlp-main.78/) 

  Wenhong Zhu, **Hongkun Hao**, Rui Wang

  ``EMNLP 2023`` \| <a href='https://github.com/hongkunhao/penalty_decoding'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="zhu-etal-2023-penalty">BIB</button>


- [Q-Refine: A Perceptual Quality Refiner for AI-Generated Image](https://arxiv.org/abs/2401.01117.pdf)
  
  Chunyi Li, Haoning Wu, Zicheng Zhang, **Hongkun Hao**, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai
  
  ``ICME 2024`` \| <a href='https://github.com/Q-Future/Q-Refine'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="li2024qrefine">BIB</button>


- [CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154)
  
  Wenhong Zhu, **Hongkun Hao**, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu
  
  ``NAACL 2024 Findings`` \| <a href='https://github.com/hongkunhao'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="zhu2024cleaneval">BIB</button>



<span class='anchor' id='preprints'></span>
# üñ®Ô∏è Preprints
\* denotes co-first authors
- [Boosting Large Language Model for Speech Synthesis: An Empirical Study](https://arxiv.org/abs/2401.00246.pdf)
  
  **Hongkun Hao**, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, Rui Wang, Furu Wei

  <button class="copy-btn" data-bib-file="hao2023boosting">BIB</button>


- [Improving Open-Ended Text Generation via Adaptive Decoding](https://arxiv.org/abs/2402.18223)
  
  Wenhong Zhu, **Hongkun Hao**, Zhiwei He, Yiming Ai, Rui Wang

  <a href='https://github.com/hongkunhao/adaptive_decoding'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="zhu2024improving">BIB</button>


- [Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models](https://arxiv.org/abs/2402.14007)

  Zhiwei He\*, Binglin Zhou\*, **Hongkun Hao**, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang

  <a href='https://github.com/zwhe99/X-SIR'><button class="code-btn">CODE</button></a> <button class="copy-btn" data-bib-file="he2024can">BIB</button> <a href='https://cross-lingual-watermark.github.io/'><button class="home-btn">HOME</button></a>


- [Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality](https://arxiv.org/abs/2402.14679)

  Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, **Hongkun Hao**, Kai Yu, Lingjun Chen, Rui Wang

  <button class="copy-btn" data-bib-file="ai2024cognition">BIB</button>


- [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656)

  Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, **Hongkun Hao**, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei

  <button class="copy-btn" data-bib-file="hu2024wavllm">BIB</button>



# üíª Internships
- *2024.01 - present*: [ByteDance], Shanghai, China.
- *2023.04 - 2023.11*: [Microsoft Research Asia], Beijing, China.
- *2022.12 - 2023.01*: [Tencent AI Lab](https://ai.tencent.com/ailab/en/index), Shenzhen, China.



# üéñ Honors and Awards
- *2023.08*: Optiver Excellent Student Scholarship
- *2022.06*: Best Bachelor Thesis Award
- *2022.06*: Shanghai Jiao Tong University Outstanding Graduates
- *2019, 2020, 2021*: Academic Excellence Scholarship
- *2019*: The first prize of Mathematics Competition of Chinese College Student 

# üí¨ Invited Talks
- *2023.06*: Rethinking Translation Memory Augmented Neural Machine Translation, AIS \| [\[slide\]](talks/AIS/AIS-Spot-3.pdf)





<p align="center" style="padding-top: 40px;"><a href='https://clustrmaps.com/site/1bz3w'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=zQTIbvMowu5vzFhApfVkwyYZZQbVqasooVPCbiwEIlo'/></a></p>

